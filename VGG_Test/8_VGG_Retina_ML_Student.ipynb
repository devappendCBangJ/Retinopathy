{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVy7iNWeGpuw"
   },
   "source": [
    "## 0. 학습 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3CJJ9EnGpu4"
   },
   "source": [
    "### 1) 메모리 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2268,
     "status": "ok",
     "timestamp": 1670137745030,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "ELbd4K-MGpu5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHYzhQ27Gpu7"
   },
   "source": [
    "### 2) 수정된 코드 자동 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1670093614989,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "emfP9O6NGpu8",
    "outputId": "b5d5aa43-21a2-46b7-efd9-0f2093c510b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%load_ext autoreload\\n%autoreload 2\\nimport foolbox as fb'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%load_ext autoreload\n",
    "%autoreload 2\n",
    "import foolbox as fb\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPMzSWTmGpu9"
   },
   "source": [
    "## 1. Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1670137808563,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "BoJGB4yOGpu-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "\"\"\"\n",
    "import argparse\n",
    "import easydict\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "\"\"\"\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\"\"\"\n",
    "import torch.utils.data as data\n",
    "\"\"\"\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "from src.models import vgg11_config, vgg13_config, vgg16_config, ConvNet\n",
    "from src.models import VGG, get_vgg_layers\n",
    "from src.engines import train, evaluate, get_predictions\n",
    "from src.engines import plot_most_correct_wrong, epoch_time, normalize_image\n",
    "from src.utils import load_checkpoint, save_checkpoint, save_transform, save_best_param\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(10**7)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1544,
     "status": "ok",
     "timestamp": 1670137812457,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "I4y5_zn2Gpu_"
   },
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1670137816713,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "PA7I6xU9GpvB"
   },
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "grid_count = 0\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "def GridSearch_loop(trial: Trial) -> float:\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    global grid_count\n",
    "    grid_count += 1\n",
    "    print(f\"================== {grid_count}번째 Grid ==================\")\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \n",
    "    #==================================================================================================\n",
    "    # 2. Variable Declaration\n",
    "    #==================================================================================================\n",
    "    # Jupyter 환경\n",
    "    args = easydict.EasyDict({\n",
    "            \"title\" : \"VGG_Retina_ML_Grid_Student\",\n",
    "            \"load_model\" : \"VGG_Retina_ML_Grid3\",\n",
    "            \"device\" : \"cuda\",\n",
    "            \"root\" : \"data\",\n",
    "            \"use_data\" : \"Retina_Some_binary\", # Retina_Some_binary # Retina_Some_binary_GAN284 # Retina_Some_binary_GAN852 # Retina_student\n",
    "            \"batch_size\" : 32, # !!!\n",
    "            \"num_workers\" : 2,\n",
    "            \"epochs\" : 200, # !!!\n",
    "            #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "            # \"lr\" : 7.5e-5, # !!!\n",
    "            \"lr\" : trial.suggest_loguniform(\"lr\", 1e-6, 5e-3), # 1e-6, 5e-3 # 1e-6, 5e-5 !!!\n",
    "            #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "            \"logs\" : \"logs\",\n",
    "            \"checkpoints\" : \"checkpoints\",\n",
    "            \"transform_dir\" : \"transform_infor\",\n",
    "            \"resume\" : False,\n",
    "            \"train_ratio\" : 0.8,\n",
    "            \"val_ratio\" : 0.2,\n",
    "            \"test_ratio\" : 1.0,\n",
    "            \"output_dim\" : 2,\n",
    "            \"drop_rate\": 0.5\n",
    "        })\n",
    "    \n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    print(\"lr : \", args.lr)\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 3. Image Data Preprocessing\n",
    "    # 1) 이미지 변환\n",
    "    #==================================================================================================\n",
    "    # Build Dataset\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    # transform 임의 선택(1 ~ 최대 개수) + 이미지 변환\n",
    "    list_transforms = [T.RandomRotation(30), T.RandomHorizontalFlip(), T.RandomVerticalFlip(),\n",
    "                        T.RandomAutocontrast(), T.RandomPerspective(fill=[255,255,255]), T.RandomInvert(), \n",
    "                        T.RandomGrayscale(p=0.5), T.RandomEqualize(), T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)]\n",
    "    \"\"\"\n",
    "    list_transforms = [T.RandomRotation(180), T.RandomHorizontalFlip(), T.RandomVerticalFlip(),\n",
    "                    T.RandomAutocontrast(), T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 5)),\n",
    "                    T.RandomAffine(180, shear=20), T.RandomPerspective(fill=[255,255,255]),\n",
    "                    T.RandomGrayscale(p=0.5), T.RandomResizedCrop((256, 256))]\n",
    "    \"\"\"\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    list_transforms_stu = [T.RandomRotation(30), T.RandomHorizontalFlip(), T.RandomVerticalFlip(),\n",
    "                        T.RandomAutocontrast(), T.RandomPerspective(fill=[255,255,255]), T.RandomInvert(), \n",
    "                        T.RandomGrayscale(p=0.5), T.RandomEqualize(), T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)]\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    \n",
    "    # rand_number_transform = random.randint(1, len(list_transforms)-1)\n",
    "    rand_number_transform = random.randint(1, 3)\n",
    "    rand_list_transforms = random.sample(list_transforms, rand_number_transform)\n",
    "\n",
    "    train_transforms = T.Compose([\n",
    "        T.Resize((256, 256)), # 이미지 크기 재조절\n",
    "        *rand_list_transforms,\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"train_transforms : \", train_transforms)\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \"\"\"\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    train_transforms = T.Compose([\n",
    "        T.Resize((256, 256)), # 이미지 크기 재조절\n",
    "        T.RandomOrder([\n",
    "            # T.RandomRotation(5), # 이미지 회전(5도 이하)\n",
    "            # T.RandomHorizontalFlip(0.5), # 이미지 좌우 대칭(50% 확률)\n",
    "    #         T.RandomRotation(180),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomVerticalFlip(),\n",
    "            T.RandomAutocontrast(),\n",
    "    #         T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 5)),\n",
    "    #         T.RandomAffine(180, shear=20),\n",
    "    #         T.RandomPerspective(fill=[255,255,255]),\n",
    "    #         T.RandomGrayscale(p=0.5),\n",
    "    #         T.RandomResizedCrop((256, 256)),\n",
    "        ]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \"\"\"\n",
    "\n",
    "    test_transforms = T.Compose([\n",
    "        T.Resize((256, 256)), \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # transform 정보 저장\n",
    "    save_transform(args.transform_dir, train_transforms, test_transforms, args.title)\n",
    "    \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    def stu_transform(list_transforms_stu):\n",
    "        rand_number_transform_stu = random.randint(1, 3)\n",
    "        rand_list_transform_stu = random.sample(list_transforms_stu, rand_number_transform_stu)\n",
    "\n",
    "        stu_transform = T.Compose([\n",
    "            T.Resize((256, 256)), # 이미지 크기 재조절\n",
    "            *rand_list_transform_stu,\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return stu_transform\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 2) 이미지 데이터셋 불러오기 + 이미지 변환\n",
    "    #==================================================================================================\n",
    "    train_path = \"C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\data\\\\Retina_Some_binary\\\\train\" \n",
    "    test_path = \"C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\data\\\\Retina_Some_binary\\\\test\"\n",
    "    \n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        train_path, \n",
    "        transform = train_transforms\n",
    "    )\n",
    "    valid_dataset = torchvision.datasets.ImageFolder(\n",
    "        train_path, \n",
    "        transform = test_transforms\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.ImageFolder(\n",
    "        test_path, \n",
    "        transform = test_transforms\n",
    "    )\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \n",
    "    train_dataset_classes = train_dataset.classes\n",
    "    valid_dataset_classes = valid_dataset.classes\n",
    "    test_dataset_classes = test_dataset.classes\n",
    "\n",
    "    print(\"len(train_dataset) : \", len(train_dataset))\n",
    "    print(\"train_dataset_classes : \", train_dataset_classes)\n",
    "    print(\"train_dataset.__getitem__(18) : \", train_dataset.__getitem__(18))\n",
    "    \n",
    "    print(\"len(valid_dataset) : \", len(valid_dataset))\n",
    "    print(\"valid_dataset_classes : \", valid_dataset_classes)\n",
    "    print(\"valid_dataset.__getitem__(18) : \", valid_dataset.__getitem__(18))\n",
    "\n",
    "    print(\"len(test_dataset) : \", len(test_dataset))\n",
    "    print(\"test_dataset_classes : \", test_dataset_classes)\n",
    "    print(\"test_dataset.__getitem__(18) : \", test_dataset.__getitem__(18))\n",
    "    \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    stu_path = \"C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\data\\\\StudentData_GAN\"\n",
    "\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    stu_dataset1 = torchvision.datasets.ImageFolder(\n",
    "        stu_path, \n",
    "        transform = stu_transform(list_transforms_stu)\n",
    "    )\n",
    "    stu_dataset2 = torchvision.datasets.ImageFolder(\n",
    "        stu_path, \n",
    "        transform = stu_transform(list_transforms_stu)\n",
    "    )\n",
    "    stu_dataset3 = torchvision.datasets.ImageFolder(\n",
    "        stu_path, \n",
    "        transform = stu_transform(list_transforms_stu)\n",
    "    )\n",
    "    stu_dataset = stu_dataset1 # + stu_dataset2 + stu_dataset3\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    #==================================================================================================\n",
    "    # 3) Noisy Student 학습\n",
    "    #==================================================================================================\n",
    "    vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)\n",
    "    model = VGG(vgg11_layers, args.output_dim)\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\best_valid_model\\\\Retina_ML_7.5r1e-5_Aug3.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(len(stu_dataset)):\n",
    "        model.eval()\n",
    "        ima = stu_dataset[i][0].unsqueeze(0)\n",
    "        ima = ima.cuda()\n",
    "        if model(ima)[0][0] >= 0.0:\n",
    "            idx = [[1]]\n",
    "#             print(\"idx : \", idx)\n",
    "            idx[0][0] = stu_dataset[i][0]\n",
    "#             print(\"idx[0][0] : \", idx[0][0])\n",
    "#             print(\"stu_dataset[i][0] : \", stu_dataset[i][0])\n",
    "#             print(\"idx[0] : \", idx[0])\n",
    "            idx[0].append(0)\n",
    "#             print(\"idx[0] : \", idx[0])\n",
    "#             print(idx[0][0])\n",
    "#             print(idx.classes)\n",
    "            a = tuple(idx[0]) # a = tuple(idx[0])\n",
    "#             print(\"stu_dataset : \", stu_dataset)\n",
    "#             print(\"a : \", a)\n",
    "#             print(\"a[0] : \", a[0])\n",
    "#             print(\"train_dataset[0] : \", train_dataset[0])\n",
    "#             print(\"a : \", a)\n",
    "#             print(\"train_dataset : \", train_dataset)\n",
    "            train_dataset = train_dataset + a\n",
    "#             print(a.classes)\n",
    "            # print(\"len(train_dataset) : \", len(train_dataset))\n",
    "        elif model(ima)[0][0] <= 0.0:\n",
    "            idx = [[1]]\n",
    "            idx[0][0] = stu_dataset[i][0]\n",
    "            idx[0].append(1)\n",
    "            a = tuple(idx[0]) # a = tuple(idx[0])\n",
    "            train_dataset = train_dataset + a\n",
    "#             print(a.classes)\n",
    "            # print(\"len(train_dataset) : \", len(train_dataset))\n",
    "#         print(\"stu_epoch : \", i)\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "    train_dataset_classes = train_dataset.classes\n",
    "    valid_dataset_classes = valid_dataset.classes\n",
    "    test_dataset_classes = test_dataset.classes\n",
    "\n",
    "    print(\"len(train_dataset) : \", len(train_dataset))\n",
    "    print(\"train_dataset_classes : \", train_dataset_classes)\n",
    "    print(\"train_dataset.__getitem__(18) : \", train_dataset.__getitem__(18))\n",
    "    \n",
    "    print(\"len(valid_dataset) : \", len(valid_dataset))\n",
    "    print(\"valid_dataset_classes : \", valid_dataset_classes)\n",
    "    print(\"valid_dataset.__getitem__(18) : \", valid_dataset.__getitem__(18))\n",
    "\n",
    "    print(\"len(test_dataset) : \", len(test_dataset))\n",
    "    print(\"test_dataset_classes : \", test_dataset_classes)\n",
    "    print(\"test_dataset.__getitem__(18) : \", test_dataset.__getitem__(18))\n",
    "    \n",
    "    #==================================================================================================\n",
    "    # 3) 훈련, 검증 데이터 분할 + 검증 이미지 재변환\n",
    "    #==================================================================================================\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    # 데이터 인덱스 랜덤 추출\n",
    "    num_train_dataset = len(train_dataset)\n",
    "    train_dataset_indices = list(range(num_train_dataset))\n",
    "    np.random.shuffle(train_dataset_indices)\n",
    "    split_idx = int(np.floor(args.val_ratio * num_train_dataset))\n",
    "    train_idx, valid_idx = train_dataset_indices[split_idx:], train_dataset_indices[:split_idx]\n",
    "\n",
    "    # 랜덤 배치 샘플러\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \"\"\"\n",
    "    all_train_dataset_size = len(train_dataset)\n",
    "    train_dataset_size = int(all_train_dataset_size * args.train_ratio)\n",
    "    valid_dataset_size = all_train_dataset_size - train_dataset_size\n",
    "\n",
    "    splited_train_dataset, splited_valid_dataset = random_split(train_dataset, [train_dataset_size, valid_dataset_size]) # 훈련 데이터셋, 검증 데이터셋 크기 결정\n",
    "\n",
    "    splited_valid_dataset = copy.deepcopy(splited_valid_dataset)\n",
    "    splited_valid_dataset.dataset.transform = test_transforms\n",
    "    \"\"\"\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 4) 데이터 로드 to 메모리\n",
    "    #==================================================================================================\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                                batch_size = args.batch_size, \n",
    "                                sampler = train_sampler, \n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    val_loader = DataLoader(valid_dataset,\n",
    "                                batch_size = args.batch_size,\n",
    "                                sampler = valid_sampler, \n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                                batch_size = args.batch_size,\n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    \n",
    "    print(f\"train_dataset 개수 : {len(train_dataset)}, \" + f\"train_loader 개수 : {len(train_loader)}\") # train_data 개수, train_loader batch set 개수\n",
    "    print(f\"val_dataset 개수 : {len(valid_dataset)}, \" + f\"val_loader 개수 : {len(val_loader)}\") # val_data 개수, val_loader batch set 개수\n",
    "    print(f\"test_dataset 개수 : {len(test_dataset)}, \" + f\"test_loader 개수 : {len(test_loader)}\") # test_data 개수, test_loader batch set 개수\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                                shuffle = True\n",
    "                                batch_size = args.batch_size, \n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    val_loader = DataLoader(valid_dataset,\n",
    "                                batch_size = args.batch_size,\n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                                batch_size = args.batch_size,\n",
    "                                num_workers=args.num_workers\n",
    "                            )\n",
    "    \n",
    "    print(f\"train_dataset 개수 : {len(splited_train_dataset)}, \" + f\"train_loader 개수 : {len(train_loader)}\") # train_data 개수, train_loader batch set 개수\n",
    "    print(f\"val_dataset 개수 : {len(splited_valid_dataset)}, \" + f\"val_loader 개수 : {len(val_loader)}\") # val_data 개수, val_loader batch set 개수\n",
    "    print(f\"test_dataset 개수 : {len(test_dataset)}, \" + f\"test_loader 개수 : {len(test_loader)}\") # test_data 개수, test_loader batch set 개수\n",
    "    \"\"\"\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 5) 시각화\n",
    "    #==================================================================================================\n",
    "    train_loader_iter = iter(train_loader)\n",
    "    train_loader_images, train_loader_labels = train_loader_iter.next()\n",
    "\n",
    "    test_loader_iter = iter(test_loader)\n",
    "    test_loader_images, test_loader_labels = test_loader_iter.next()\n",
    "\n",
    "    def imshow(img):\n",
    "        np_img = img.numpy()\n",
    "        plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"np_img.shape : \", np_img.shape)\n",
    "        print(\"(np.transpose(np_img, (1, 2, 0))).shape : \", (np.transpose(np_img, (1, 2, 0))).shape)\n",
    "        \"\"\"\n",
    "\n",
    "    print(\"train_loader_labels : \", train_loader_labels)\n",
    "    print(\"\".join(\"%5s \"%train_dataset_classes[train_loader_labels[j]] for j in range(32)))\n",
    "    print(\"train_loader_images.shape : \", train_loader_images.shape)\n",
    "    imshow(torchvision.utils.make_grid(train_loader_images, nrow=6))\n",
    "    print(\"torchvision.utils.make_grid(train_loader_images).shape : \", torchvision.utils.make_grid(train_loader_images).shape)\n",
    "\n",
    "    print(\"test_loader_labels : \", test_loader_labels)\n",
    "    print(\"\".join(\"%5s \"%test_dataset_classes[test_loader_labels[j]] for j in range(32)))\n",
    "    print(\"test_loader_images.shape : \", test_loader_images.shape)\n",
    "    imshow(torchvision.utils.make_grid(test_loader_images, nrow=6))\n",
    "    print(\"torchvision.utils.make_grid(test_loader_images).shape : \", torchvision.utils.make_grid(test_loader_images).shape)\n",
    "    \n",
    "    #==================================================================================================\n",
    "    # 4. Model Define\n",
    "    # 1) 모델 정의\n",
    "    #==================================================================================================\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    \"\"\"\n",
    "    # Build model\n",
    "    vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)\n",
    "    model = VGG(vgg11_layers, args.output_dim)\n",
    "    \"\"\"\n",
    "    #model = ConvNet(drop_rate=args.drop_rate)\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    #model = model.to(args.device)\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 2) 옵티마이저 + 손실함수 + 스케쥴러 + 메트릭 함수 정의\n",
    "    #==================================================================================================\n",
    "    # Build optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Build scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs * len(train_loader))\n",
    "\n",
    "    # Build loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Build metric function\n",
    "    \"\"\"\n",
    "    # 정확도 측정 함수\n",
    "    def calculate_accuracy(y_pred, y): # ???\n",
    "        top_pred = y_pred.argmax(1, keepdim = True)\n",
    "        # print(\"top_pred : \", top_pred)\n",
    "        correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "        # print(\"correct : \", correct)\n",
    "        acc = correct.float() / y.shape[0]\n",
    "        # print(\"acc : \", acc)\n",
    "        return acc\n",
    "    \"\"\"\n",
    "    metric_fn = accuracy\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 3) logger 정의\n",
    "    #==================================================================================================\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "    # Build logger\n",
    "    train_logger = SummaryWriter(f'{args.logs}/train/{args.title}_{grid_count}')\n",
    "    val_logger = SummaryWriter(f'{args.logs}/val/{args.title}_{grid_count}')\n",
    "    test_logger = SummaryWriter(f'{args.logs}/test/{args.title}_{grid_count}')\n",
    "    #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 5. Model Train\n",
    "    # 1) Load model epoch\n",
    "    #==================================================================================================\n",
    "    # Load model\n",
    "    start_epoch = 0\n",
    "    if args.resume:\n",
    "        start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 2) Train model\n",
    "    #==================================================================================================\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        \"\"\"\n",
    "        # start timer\n",
    "        start_time = time.time() # 확인용 코드\n",
    "        \"\"\"\n",
    "        # 모델 학습 소요시간\n",
    "        start_time = time.monotonic()\n",
    "\n",
    "        # train one epoch + evaluate one epoch\n",
    "        train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
    "        val_summary = evaluate(val_loader, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "        # write log\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        train_logger.add_scalar(f'Loss_{args.lr}', train_summary['loss'], epoch + 1)\n",
    "        train_logger.add_scalar(f'Accuracy_{args.lr}', train_summary['metric'], epoch + 1)\n",
    "        val_logger.add_scalar(f'Loss_{args.lr}', val_summary['loss'], epoch + 1)\n",
    "        val_logger.add_scalar(f'Accuracy_{args.lr}', val_summary['metric'], epoch + 1)\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        \"\"\"\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "        val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "        val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        \"\"\"\n",
    "\n",
    "        # 최적 loss인 model 저장\n",
    "        global best_valid_loss\n",
    "        if val_summary['loss'] < best_valid_loss:\n",
    "            best_valid_loss = val_summary['loss']\n",
    "            #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "            torch.save(model.state_dict(), f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\best_valid_model\\\\{args.title}_{grid_count}.pt')\n",
    "            save_best_param(args.transform_dir, train_transforms, test_transforms, args.lr, args.title)\n",
    "        torch.save(model.state_dict(), f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\best_valid_model\\\\{args.title}_{grid_count}.pt')\n",
    "            #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "\n",
    "        \"\"\"\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        # save model\n",
    "        save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
    "        #ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "        \"\"\"\n",
    "\n",
    "        # 모델 학습 소요시간\n",
    "        end_time = time.monotonic()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        # Print log\n",
    "        \"\"\"\n",
    "        print((\n",
    "            f'[Epoch {epoch+1}] '\n",
    "            + f'{epoch + 1}epoch time {end_time - start_time:.02f}, '\n",
    "            + f'Train Loss {train_summary[\"loss\"]:.04f}, '\n",
    "            + f'Train Accuracy {train_summary[\"metric\"]:.04f}, '\n",
    "            + f'Test Loss {val_summary[\"loss\"]:.04f}, '\n",
    "            + f'Test Accuracy {val_summary[\"metric\"]:.04f}'\n",
    "        ))\n",
    "        \"\"\"\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\t Train Loss: {train_summary[\"loss\"]:.3f} | Train Acc: {train_summary[\"metric\"]:.2f}%')\n",
    "        print(f'\\t Valid Loss: {val_summary[\"loss\"]:.3f} | Valid Acc: {val_summary[\"metric\"]:.2f}%')\n",
    "        print(f'\\t scheduled_lr : {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 6. Model Test\n",
    "    #==================================================================================================\n",
    "    # 학습된 모델 불러오기\n",
    "    model.load_state_dict(torch.load(f'C:\\\\Users\\\\Bang\\\\JupyterProjects\\\\RetinaProject\\\\DeepLearningPytorchExample\\\\best_valid_model\\\\{args.title}_{grid_count}.pt'))\n",
    "\n",
    "    # 모델 성능 측정\n",
    "    test_summary = evaluate(test_loader, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "    # write log\n",
    "    test_logger.add_scalar('Loss', test_summary['loss'], epoch + 1)\n",
    "    test_logger.add_scalar('Accuracy', test_summary['metric'], epoch + 1)\n",
    "\n",
    "    print(f'Test Loss: {test_summary[\"loss\"]:.3f} | Test Acc: {test_summary[\"metric\"]:.2f}%')\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 7. Model Prediction Print\n",
    "    # 1) 예측한 값 추출 + 맞춘 이미지 정보 추출 + 틀린 이미지 정보 추출\n",
    "    #==================================================================================================\n",
    "    correct_examples, wrong_examples = get_predictions(test_loader, model, args.device)\n",
    "\n",
    "#     for correct_example in correct_examples: # 확인용 코드\n",
    "#         print(\"correct_example : \", correct_example[2])\n",
    "#     for wrong_example in wrong_examples: # 확인용 코드\n",
    "#         print(\"wrong_example : \", wrong_example[2])\n",
    "\n",
    "    classes = test_dataset.classes\n",
    "    n_images = 5\n",
    "    plot_most_correct_wrong(correct_examples, wrong_examples, classes, n_images)\n",
    "\n",
    "    #==================================================================================================\n",
    "    # 3) return loss\n",
    "    #==================================================================================================\n",
    "    return val_summary[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 551922,
     "status": "error",
     "timestamp": 1670138374532,
     "user": {
      "displayName": "추제근",
      "userId": "03658758388761689109"
     },
     "user_tz": -540
    },
    "id": "txXYkHloGpvI",
    "outputId": "e8cd8048-c73f-456c-bfdd-328b90134efb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-04 20:03:11,644]\u001b[0m A new study created in memory with name: Retina_opt\u001b[0m\n",
      "C:\\Users\\Bang\\AppData\\Local\\Temp\\ipykernel_11012\\1251303250.py:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  \"lr\" : trial.suggest_loguniform(\"lr\", 1e-6, 5e-3), # 1e-6, 5e-3 # 1e-6, 5e-5 !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== 1번째 Grid ==================\n",
      "lr :  2.428916946974887e-05\n",
      "train_transforms :  Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear)\n",
      "    RandomAutocontrast(p=0.5)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomInvert(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "len(train_dataset) :  4532\n",
      "train_dataset_classes :  ['DR', 'No_DR']\n",
      "train_dataset.__getitem__(18) :  (tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         ...,\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
      "         [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
      "\n",
      "        [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         ...,\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
      "         [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
      "\n",
      "        [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         ...,\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
      "         [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]), 0)\n",
      "len(valid_dataset) :  4532\n",
      "valid_dataset_classes :  ['DR', 'No_DR']\n",
      "valid_dataset.__getitem__(18) :  (tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "len(test_dataset) :  1132\n",
      "test_dataset_classes :  ['DR', 'No_DR']\n",
      "test_dataset.__getitem__(18) :  (tensor([[[-2.1179, -2.1008, -2.1008,  ..., -2.1179, -2.1008, -2.0837],\n",
      "         [-2.1008, -2.1008, -2.1008,  ..., -2.1179, -2.1008, -2.0665],\n",
      "         [-2.1179, -2.1008, -2.1008,  ..., -2.1179, -2.1008, -2.0837],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.0837, -2.0494],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1008, -2.0665],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1008, -2.0665, -2.1008]],\n",
      "\n",
      "        [[-2.0007, -2.0182, -2.0182,  ..., -1.9832, -1.9832, -2.0007],\n",
      "         [-2.0182, -1.9832, -2.0007,  ..., -2.0182, -2.0007, -1.9832],\n",
      "         [-2.0007, -2.0007, -2.0182,  ..., -1.9832, -1.9832, -1.9657],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0182,  ..., -1.9482, -1.9482, -1.9307],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -1.9657, -1.9657, -1.9482],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -1.9657, -1.9657, -1.9482]],\n",
      "\n",
      "        [[-1.7870, -1.7870, -1.7870,  ..., -1.8044, -1.7870, -1.7696],\n",
      "         [-1.7870, -1.8044, -1.7870,  ..., -1.7696, -1.7696, -1.7522],\n",
      "         [-1.8044, -1.7870, -1.7870,  ..., -1.7870, -1.7870, -1.7696],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.7870,  ..., -1.7696, -1.7347, -1.7522],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.7522, -1.7347, -1.7696],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.7522, -1.7522, -1.7696]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "# GridSearch loop\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"Retina_opt\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "study.optimize(GridSearch_loop, n_trials=1)\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best trial:\", study.best_trial.params)\n",
    "#ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
