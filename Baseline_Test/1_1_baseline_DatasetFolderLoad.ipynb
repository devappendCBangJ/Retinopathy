{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "import easydict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchmetrics.aggregation import MeanMetric\n",
        "from torchmetrics.functional.classification import accuracy\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from src.models import ConvNet\n",
        "from src.engines import train, evaluate\n",
        "from src.utils import load_checkpoint, save_checkpoint\n",
        "\n",
        "# # Jupyter \uc678 \ud658\uacbd\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument(\"--title\", type=str, default=\"baseline\")\n",
        "# parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
        "# parser.add_argument(\"--root\", type=str, default=\"data\")\n",
        "# parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "# parser.add_argument(\"--num_workers\", type=int, default=2)\n",
        "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
        "# parser.add_argument(\"--lr\", type=float, default=0.001)\n",
        "# parser.add_argument(\"--logs\", type=str, default='logs')\n",
        "# parser.add_argument(\"--checkpoints\", type=str, default='checkpoints')\n",
        "# parser.add_argument(\"--resume\", type=bool, default=False)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# Jupyter \ud658\uacbd\n",
        "args = easydict.EasyDict({\n",
        "        \"title\" : \"baseline\",\n",
        "        \"device\" : \"cuda\",\n",
        "        \"root\" : \"data\",\n",
        "        \"batch_size\" : 64,\n",
        "        \"num_workers\" : 2,\n",
        "        \"epochs\" : 100,\n",
        "        \"lr\" : 0.001,\n",
        "        \"logs\" : \"logs\",\n",
        "        \"checkpoints\" : \"checkpoints\",\n",
        "        \"resume\" : False\n",
        "    })\n",
        "\n",
        "class RetinaDataset(Dataset):\n",
        "    # image dataset \uc804\uccb4 \uacbd\ub85c \uc800\uc7a5 -> tranform\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(Retina, self).__init__()\n",
        "        self.make_dataset(root)\n",
        "        self.transform = transform\n",
        "    \n",
        "    # image dataset \uc804\uccb4 \uacbd\ub85c \uc800\uc7a5\n",
        "    def make_dataset(self, root):\n",
        "        # class(\ud3f4\ub354\uba85) \ubd88\ub7ec\uc624\uae30\n",
        "        self.data = []\n",
        "        categories = os.listdir(root)\n",
        "        categories = sorted(categories)\n",
        "        \n",
        "        # class -> label \ubcc0\ud658 + \uac01 class\uc758 \uc774\ubbf8\uc9c0 \ud30c\uc77c \uc804\ubd80 \uac00\uc838\uc624\uae30\n",
        "        for label, category in enumerate(categories):\n",
        "            images = glob.glob(f'{root}/{category}/*.png')\n",
        "            for image in images:\n",
        "                self.data.append((image, label))\n",
        "    \n",
        "    # data \uac1c\uc218\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    # \uacbd\ub85c\uc5d0 \uc788\ub294, \uc9c0\uc815\ud55c idx\uc758 \uc774\ubbf8\uc9c0 \uc77d\uae30 -> RGB \ubcc0\ud658 -> tranform -> image, label \ubc18\ud658\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        image = self.read_image(image)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    # \uacbd\ub85c\uc5d0 \uc788\ub294 image \uc77d\uae30 -> RGB \ubcc0\ud658\n",
        "    def read_image(self, path):\n",
        "        image = Image.open(path)\n",
        "        return image.convert('RGB')\n",
        "\n",
        "def main(args):\n",
        "    # Build dataset\n",
        "    # - load train dataset + make loader\n",
        "    train_transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    train_root = 'data/Cifar10/train'\n",
        "    train_data = RetinaDataset(train_root, train_transform)\n",
        "    # train_data = CIFAR10(args.root, train=True, download=True, transform=train_transform)\n",
        "    train_loader = DataLoader(train_data, args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
        "\n",
        "    # - load val dataset + make loader\n",
        "    val_transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    val_root = 'data/Cifar10/test' # test!!! val!!!\n",
        "    val_data = RetinaDataset(val_root, val_transform)\n",
        "    # val_data = CIFAR10(args.root, train=False, download=True, transform=val_transform)\n",
        "    val_loader = DataLoader(val_data, batch_size=args.batch_size, num_workers=args.num_workers)\n",
        "    \n",
        "    # \ud655\uc778\uc6a9 \ucf54\ub4dc\n",
        "    print(f\"train_data \uac1c\uc218 : {len(train_data)}, \" + f\"train_loader \uac1c\uc218 : {len(train_loader)}\") # train_data \uac1c\uc218, train_loader batch set \uac1c\uc218\n",
        "    print(f\"val_data \uac1c\uc218 : {len(val_data)}, \" + f\"val_loader \uac1c\uc218 : {len(val_loader)}\") # train_data \uac1c\uc218, train_loader batch set \uac1c\uc218\n",
        "\n",
        "    # Build model\n",
        "    model = ConvNet()\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    # Build optimizer \n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Build scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs * len(train_loader))\n",
        "\n",
        "    # Build loss function\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Build metric function\n",
        "    metric_fn = accuracy\n",
        "\n",
        "    # Build logger\n",
        "    train_logger = SummaryWriter(f'{args.logs}/train/{args.title}')\n",
        "    val_logger = SummaryWriter(f'{args.logs}/val/{args.title}')\n",
        "\n",
        "    # Load model\n",
        "    start_epoch = 0\n",
        "    if args.resume:\n",
        "        start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)\n",
        "    \n",
        "    # Main loop\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        # train one epoch\n",
        "        train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
        "        \n",
        "        # evaluate one epoch\n",
        "        val_summary = evaluate(val_loader, model, loss_fn, metric_fn, args.device)\n",
        "\n",
        "        # write log\n",
        "        train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
        "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
        "        val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
        "        val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
        "\n",
        "        # save model\n",
        "        save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
        "        \n",
        "        # Print log\n",
        "        print((\n",
        "            f'Epoch {epoch + 1}: '\n",
        "            + f'Train Loss {train_summary[\"loss\"]:.04f}, '\n",
        "            + f'Train Accuracy {train_summary[\"metric\"]:.04f}, '\n",
        "            + f'Test Loss {val_summary[\"loss\"]:.04f}, '\n",
        "            + f'Test Accuracy {val_summary[\"metric\"]:.04f}'\n",
        "        ))\n",
        "        \n",
        "        break # \ud14c\uc2a4\ud2b8\u25cf\u25cf\u25cf\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main(args)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}