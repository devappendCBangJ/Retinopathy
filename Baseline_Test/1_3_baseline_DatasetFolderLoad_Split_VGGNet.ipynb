{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset load complete]\n",
      "Training Data Size : 24588\n",
      "Validation Data Size : 5268\n",
      "Testing Data Size : 5270\n",
      "train_dataset 개수 : 24588, train_loader 개수 : 384\n",
      "val_dataset 개수 : 5268, val_loader 개수 : 82\n",
      "test_dataset 개수 : 5270, test_loader 개수 : 83\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "import easydict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchmetrics.aggregation import MeanMetric\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from src_VGGNet.models import ConvNet\n",
    "from src_VGGNet.engines import train, evaluate\n",
    "from src_VGGNet.utils import load_checkpoint, save_checkpoint\n",
    "\n",
    "# # Jupyter 외 환경\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--title\", type=str, default=\"baseline\")\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "# parser.add_argument(\"--root\", type=str, default=\"data\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "# parser.add_argument(\"--num_workers\", type=int, default=2)\n",
    "# parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "# parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "# parser.add_argument(\"--logs\", type=str, default='logs')\n",
    "# parser.add_argument(\"--checkpoints\", type=str, default='checkpoints')\n",
    "# parser.add_argument(\"--resume\", type=bool, default=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Jupyter 환경\n",
    "args = easydict.EasyDict({\n",
    "        \"title\" : \"1_3_baseline_DatasetFolderLoad_Split_VGGNet\",\n",
    "        \"device\" : \"cuda\",\n",
    "        \"root\" : \"data\",\n",
    "        \"batch_size\" : 64,\n",
    "        \"num_workers\" : 2,\n",
    "        \"epochs\" : 10,\n",
    "        \"lr\" : 0.001,\n",
    "        \"logs\" : \"logs\",\n",
    "        \"checkpoints\" : \"checkpoints\",\n",
    "        \"resume\" : False,\n",
    "        \"train_ratio\" : 0.7,\n",
    "        \"val_ratio\" : 0.15,\n",
    "        \"test_ratio\" : 0.15\n",
    "    })\n",
    "\n",
    "# Build Dataset\n",
    "class RetinaDataset(Dataset):\n",
    "    # image dataset 전체 경로 저장 -> tranform\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(RetinaDataset, self).__init__()\n",
    "        self.make_dataset(root)\n",
    "        self.transform = transform\n",
    "    \n",
    "    # image dataset 전체 경로 저장\n",
    "    def make_dataset(self, root):\n",
    "        # class(폴더명) 불러오기\n",
    "        self.data = []\n",
    "        categories = os.listdir(root)\n",
    "        categories = sorted(categories)\n",
    "        \n",
    "        # class -> label 변환 + 각 class의 이미지 파일 전부 가져오기\n",
    "        for label, category in enumerate(categories):\n",
    "            images = glob.glob(f'{root}/{category}/*.png')\n",
    "            for image in images:\n",
    "                self.data.append((image, label))\n",
    "    \n",
    "    # data 개수\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # 경로에 있는, 지정한 idx의 이미지 읽기 -> RGB 변환 -> tranform -> image, label 반환\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        image = self.read_image(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    # 경로에 있는 image 읽기 -> RGB 변환\n",
    "    def read_image(self, path):\n",
    "        image = Image.open(path)\n",
    "        return image.convert('RGB')\n",
    "\n",
    "def main(args):\n",
    "    # Build dataset\n",
    "    # train, val, test dataset load + make loader\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]) # transform을 train과 test 따로따로 설정해줘야하나?\n",
    "    dataset_root = \"data/Retina\"\n",
    "    dataset = RetinaDataset(dataset_root, transform)\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(dataset_size * args.train_ratio)\n",
    "    val_size = int(dataset_size * args.val_ratio)\n",
    "    test_size = dataset_size - train_size - val_size # random_split에서 dataset_size = train_size + val_size + test_size가 되지 않으면 오류 발생\n",
    "    \n",
    "    print(\"[dataset load complete]\") # 확인용 코드\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # 확인용 코드\n",
    "    print(f\"Training Data Size : {len(train_dataset)}\")\n",
    "    print(f\"Validation Data Size : {len(val_dataset)}\")\n",
    "    print(f\"Testing Data Size : {len(test_dataset)}\")\n",
    "    \n",
    "    print(f\"train_dataset 개수 : {len(train_dataset)}, \" + f\"train_loader 개수 : {len(train_loader)}\") # train_data 개수, train_loader batch set 개수\n",
    "    print(f\"val_dataset 개수 : {len(val_dataset)}, \" + f\"val_loader 개수 : {len(val_loader)}\") # val_data 개수, val_loader batch set 개수\n",
    "    print(f\"test_dataset 개수 : {len(test_dataset)}, \" + f\"test_loader 개수 : {len(test_loader)}\") # test_data 개수, test_loader batch set 개수\n",
    "\n",
    "    # Build model\n",
    "    model = ConvNet()\n",
    "    model = model.to(args.device)\n",
    "    \n",
    "    # Build optimizer \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Build scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs * len(train_loader))\n",
    "\n",
    "    # Build loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Build metric function\n",
    "    metric_fn = accuracy\n",
    "\n",
    "    # Build logger\n",
    "    train_logger = SummaryWriter(f'{args.logs}/train/{args.title}')\n",
    "    val_logger = SummaryWriter(f'{args.logs}/val/{args.title}')\n",
    "\n",
    "    # Load model\n",
    "    start_epoch = 0\n",
    "    if args.resume:\n",
    "        start_epoch = load_checkpoint(args.checkpoints, args.title, model, optimizer)\n",
    "    \n",
    "    # Main loop\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        # start timer\n",
    "        start_time = time.time() # 확인용 코드\n",
    "\n",
    "        # train one epoch\n",
    "        train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, args.device)\n",
    "        \n",
    "        # evaluate one epoch\n",
    "        val_summary = evaluate(val_loader, model, loss_fn, metric_fn, args.device)\n",
    "\n",
    "        # write log\n",
    "        train_logger.add_scalar('Loss', train_summary['loss'], epoch + 1)\n",
    "        train_logger.add_scalar('Accuracy', train_summary['metric'], epoch + 1)\n",
    "        val_logger.add_scalar('Loss', val_summary['loss'], epoch + 1)\n",
    "        val_logger.add_scalar('Accuracy', val_summary['metric'], epoch + 1)\n",
    "\n",
    "        # save model\n",
    "        save_checkpoint(args.checkpoints, args.title, model, optimizer, epoch + 1)\n",
    "\n",
    "        # stop timer\n",
    "        end_time = time.time() # 확인용 코드\n",
    "        \n",
    "        # Print log\n",
    "        print((\n",
    "            f'[Epoch {epoch + 1}] '\n",
    "            + f'{epoch + 1}epoch time {end_time - start_time:.02f}, '\n",
    "            + f'Train Loss {train_summary[\"loss\"]:.04f}, '\n",
    "            + f'Train Accuracy {train_summary[\"metric\"]:.04f}, '\n",
    "            + f'Test Loss {val_summary[\"loss\"]:.04f}, '\n",
    "            + f'Test Accuracy {val_summary[\"metric\"]:.04f}'\n",
    "        ))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "BangEnv",
   "language": "python",
   "name": "bangenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
